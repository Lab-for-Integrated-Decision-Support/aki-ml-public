{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "source": [
    "## AKI Calculation - Description\n",
    "\n",
    "This notebook is our first attempt to identify the subjects with AKI in our cohort, based on the KDIGO SCr criteria.\n",
    "\n",
    "The code to calculate AKI within the VPS-PN cohort can be found here: https://github.com/Lab-for-Integrated-Decision-Support/vps-peds-mods. The relevant code and folders are:\n",
    "\n",
    "- r-markdown/AKI_over_3_day_LOS.Rmd - Processing markdown for AKI over the first 72 hrs (3 days) of ICU admission\n",
    "- scripts/f_aki_functions.R - R file containing function definitions\n",
    "\n",
    "**Note** This was copied over from Synapse Analytics workspace AKI Calc on 2024-01-10. None of the Spark functions were converted to Python yet.\n",
    "\n",
    "The general process is as follows:\n",
    "\n",
    "- Load the necessary data files, including ICU admissions, hospital admissions, laboratory results, and vial signs (mostly in `join_data(...)` function in the R code)\n",
    "- Select, clean, and join the data files as needed\n",
    "  - Remove those with a LOS < 12 hours, as we are calculating risk at 12 hrs\n",
    "  - Ensure the weight and height are within the proper distribution\n",
    "  - Ensure the creatinines are within the proper distribution\n",
    "  - Filter for age < 18 \n",
    "- Compute the baseline SCr (look in the aki_compute_bscr(...) function in the R code)\n",
    "  - Requires functions for height-dependent and height-independent from Hessey 2017 article\n",
    "  - Also requires Schwartz min/max estimates for ensuring we are within an appropriate range\n",
    "  - Use logic for computing based on lines ~ 360 - 375 in f_aki_functions.R script\n",
    "\n",
    "- From that, compute the AKI stage for each SCr that resulted between 0 and 72 hrs of hospital admission\n",
    "  - Use KDIGO SCr criteria, found on lines 430 - 435 of f_aki_functions.R file\n",
    "  - If AKI > stage 0 (no AKI) before 12 hrs, then patient is excluded for having AKI on admisison\n",
    "  - Otherwise, look for highest stage achieved between 12 and 72 hrs - this is our stage label for that patient\n",
    "- Summarize AKI presence in cohort and mortality among those with AKI at 72 hrs\n",
    "- Gather covariates - recall that for the original model, this includes \"cardiac arrest pre-admission\" which we will have to get from diagnoses codes, and \"post-operative\" which we likely cannot include.\n",
    "- Calculate AKI risk based on the Sanchez-Pinto 2016 coefficients on the covariates\n",
    "  - Generate an ROC curve based on this score and report the AUROC\n",
    "  - Calculate the thresholds at 50% and 90% and report results\n",
    "\n",
    "### Initialize and Load Functions\n",
    "\n",
    "First we set the folder path, and define the function `load_data_file` to load the specific PDC data file from this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1709575068518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import os, sys, importlib \n",
    "\n",
    "try:\n",
    "    import pyspark\n",
    "    import pyspark.pandas as ps\n",
    "except:\n",
    "    %pip install pyspark\n",
    "    import pyspark\n",
    "    import pyspark.pandas as ps\n",
    "\n",
    "try:\n",
    "    import plotly.express as px\n",
    "except:\n",
    "    %pip install plotly\n",
    "    import plotly.express as px\n",
    "\n",
    "# try:\n",
    "#     import spark\n",
    "# except:\n",
    "#     %pip install spark\n",
    "#     import spark\n",
    "\n",
    "# import data_utils (customized library that write or delete files in blob storage) \n",
    "# import data_utils\n",
    "import azure_BlobStorageConnection\n",
    "\n",
    "\n",
    "#importlib.reload(data_utils) # Not needed unless we are modifying this code in this session\n",
    "\n",
    "# import my load utilities\n",
    "import load_utils\n",
    "\n",
    "#importlib.reload(load_utils) # Not needed unless we modify this code in this session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set environment variables using script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from set_env_vars import set_all_env_vars\n",
    "\n",
    "set_all_env_vars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Add global `debug_` variables to determine when to print out counts, show data tables, and plot graphs. This speeds processing in the PySpark environment, but may not matter if we're working solely with pandas data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1709575068657
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "debug_count = True\n",
    "debug_show = True\n",
    "debug_plot = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Initialize spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1709575073535
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/anaconda/envs/azureml_py38/lib/python3.8/site-packages/pyspark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/anaconda/envs/azureml_py38/lib/python3.8/site-packages/pyspark/jars/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/23 17:45:56 WARN Utils: Your hostname, sdrury-compute resolves to a loopback address: 127.0.0.1; using 10.0.0.4 instead (on interface eth0)\n",
      "24/09/23 17:45:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/23 17:45:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName('creatinine').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Load pandas data frames of the four necessary data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\n",
      "Getting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5360/409669761.py:5: DtypeWarning: Columns (6,9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  labs_lim = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "import azureml.fsspec\n",
    "\n",
    "uri = 'azureml://subscriptions/b06a3f28-16e7-4bcd-ab95-f192daca58db/resourcegroups/pdc-rg-westus-research/workspaces/pdc-mlw-westus-research1/datastores/pdc_ci_westus_research_readwrite_iam_link/paths/meds-wg/'\n",
    "\n",
    "labs_lim = pd.read_csv(\n",
    "    uri + 'labs.csv', \n",
    "    usecols=[\"pdc_hid\", \"pdc_pid\", \"resulttime\", \"pdc_name\", \"source_units\", \"source_value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ALBUMIN', 'ALP', 'ALT', 'AST', 'BUN', 'CALCIUM_TOT',\n",
       "       'CALCIUM_ION', 'CHLORIDE', 'FIBRINOGEN', 'GLUCOSE', 'MAGNESIUM',\n",
       "       nan, 'MCV', 'POTASSIUM', 'PT', 'PTT', 'SODIUM', 'WBC',\n",
       "       'HEMOGLOBIN', 'PLTS', 'RDW', 'LYMPHO_PCT', 'NEUTRO_PCT',\n",
       "       'BANDS_PCT', 'BILIRUBIN_TOT', 'BICARBONATE', 'PROTEIN', 'INR',\n",
       "       'PH_ART', 'PCO2_ART', 'PO2_ART', 'BASE_EXC', 'PH_VEN', 'PCO2_VEN',\n",
       "       'PO2_VEN', 'CREATININE', 'LACTATE', 'CRP', 'PH_CAP', 'PCO2_CAP',\n",
       "       'PO2_CAP', 'CULT_URINE', 'GGT', 'PHOSPHORUS', 'CULT_OTHER',\n",
       "       'CULT_BLOOD', 'INFLUENZA_A', 'INFLUENZA_B', 'PARAINFLU',\n",
       "       'RHINO_ENTERO', 'HMPV', 'ADENOVIRUS', 'CULT_RESP', 'LDH',\n",
       "       'BILIRUBIN_DIR', 'FERRITIN', 'ESR', 'LIPASE', 'AMYLASE',\n",
       "       'CULT_CSF', 'RESP_VIRAL_PCR', 'ANC', 'ALC', 'HGB', 'D_DIMER',\n",
       "       'PROCALCITONIN', 'PCO2', 'BANDS_ABS', 'CORONAVIRUS', 'RSV',\n",
       "       'CORTISOL', 'SARS_COV_2', 'INFLUENZA', 'CULTURE', 'BASE_DEF'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labs_lim['pdc_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load `base_cohort` and `height_join_lim_mod` dataframe from previous notebook\n",
    "\n",
    "`base_cohort` is a dataframe created in notebook `aki_calc_base_cohort.ipynb` that describes the patients in our cohort. We will use this dataframe to select only relevant rows from larger databases to be used in later calculations.\n",
    "\n",
    "`height_join_lim_mod` is a dataframe containing information about the heights of patients in our cohort.\n",
    "\n",
    "Note that `base_cohort.csv` should be in the same directory that holds the `vps-peds-aki` repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1709575257034
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_cohort = pd.read_csv(os.environ['AKI_DATA_PDC'] + 'base_cohort_v2.csv')\n",
    "height_join_lim_mod = spark.read.parquet('../../height_join_lim_mod.parquet')\n",
    "\n",
    "sites = base_cohort['site_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "source": [
    "### Identify Creatinine Values\n",
    "\n",
    "Next we pull out the creatinine values from the `labs_lim` data frame, restrict to `pdc_hid` values in the base cohort, and limit to those which resulted between the ICU Start and ICU End times. Because each row in `base_cohort` does not necessarily have a unique `pcd_hid` (a patient may be admitted to the ICU several times during a hospitalization), we have to left join with the base_cohort `pcd_hid` and then limit by the times to find the `pcd_eid` curresponding to that SCr lab value result.\n",
    "\n",
    "**Note:** We are NOT limiting by ICU start and end time, because we will need SCr values from prior to the ICU start time to consider for the baseline SCr values.\n",
    "\n",
    "**Also note:** There are many values in the `source_value` column that do not correspond to a single number (e.g. 'N/A', '< 0.02'). For now these will be dropped, but later a method for obtaining numeric values for some of these may be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1709575341929
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Filter the labs to the SCr \n",
    "labs_scr = labs_lim[labs_lim['pdc_name']=='CREATININE']\n",
    "\n",
    "def get_float_if_numeric_str(str_in):\n",
    "    try:\n",
    "        return float(str_in)\n",
    "    except:\n",
    "        return numpy.nan\n",
    "    \n",
    "labs_scr['val_numeric'] = [get_float_if_numeric_str(value) for value in labs_scr['source_value']]\n",
    "labs_scr = labs_scr[~labs_scr['val_numeric'].isna()]\n",
    "labs_scr['n_res_time'] = labs_scr['resulttime'].astype('long')\n",
    "labs_scr = labs_scr.drop_duplicates()\n",
    "\n",
    "if debug_count: print('\\nNumber of rows of all Scr values:',labs_scr.count())\n",
    "\n",
    "# Join to base cohort\n",
    "scr_join_left = (\n",
    "    base_cohort \\\n",
    "    .merge(labs_scr, on=['pdc_hid', 'pdc_pid'], how='left') \\\n",
    "    .drop_duplicates())\n",
    "if debug_count: print('\\nNumber of rows in left join:',scr_join_left.count())\n",
    "\n",
    "# Filter to include only those with the time in between the ICU admission and discharge time\n",
    "scr_join_left_icu = scr_join_left[\n",
    "    (scr_join_left['n_res_time'] > scr_join_left['n_icuadmit']) \\\n",
    "    & (scr_join_left['n_res_time'] <= scr_join_left['n_icudc'])\n",
    "]\n",
    "if debug_count: print(\\n'Number of filtered rows in ICU-only dataset:',scr_join_left_icu.count())\n",
    "\n",
    "if debug_show: scr_join_left.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now we make a box plot of the ICU SCr values by site, to ensure they appear similar (we cannot plot the full SCr dataset because the memory requirements are too great for the KL2Small machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1709575300418
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if debug_plot:\n",
    "\n",
    "    fig = px.box(scr_join_left_icu[[\"site_id\", \"val_numeric\"]], x=\"site_id\", y=\"val_numeric\", \n",
    "        labels={\n",
    "            \"siteid\": \"Site ID\",\n",
    "            \"val_numeric\": \"Serum Creatinine Value\"\n",
    "        },\n",
    "        log_y=True)\n",
    "    fig.update_xaxes(type='category')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Baseline Creatinine\n",
    "\n",
    "Now that we have SCr and height (for a subset of rows in our `base_cohort`), we have to calculate the baseline creatinine. This is done based on the following order:\n",
    "\n",
    "- Prior creatinine within 6 months (up to 1 cay - can't use 12 hrs due to granularity) of ICU admission, as long as it falls above the lower limit of the Schwartz values (Mean - 2*SD)\n",
    "- Height-dependent formula, if a height is given\n",
    "- Height-independent formula, if no height is given\n",
    "\n",
    "Recall that our data frame for creatinine values, `scr_join_left`, has all SCr values along with an associated result time (seconds from admission), age at admission, and patient ID. We need to collect all creatinines associated with each PID and the age at result (days will be the most granular we can get), then for each tuple (`hid`, `eid`) we look back 6 months through 1 day prior and attempt to identify the most recent baseline creatinine. The result may be None if there are no SCr values meeting that date restrictor for that tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1709575388929
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# First we generate the table by PID which contains the creatinine values and \n",
    "# age (fraction of days)\n",
    "\n",
    "scr_dates = scr_join_left[[\n",
    "    \"pdc_pid\", \"ageatadmission\", \"val_numeric\", \"n_res_time\"\n",
    "]]\n",
    "\n",
    "scr_dates = scr_dates[\n",
    "    (~scr_dates['val_numeric'].isna()) & \\\n",
    "    (~scr_dates['ageatadmission'].isna())\n",
    "]\n",
    "\n",
    "scr_dates['n_age_days'] = scr_dates['ageatadmission'].astype('long')\n",
    "scr_dates['age_res_time'] = scr_dates['n_age_days'] + (scr_dates['n_res_time'] * (60. * 60. * 24.))\n",
    "\n",
    "scr_dates = scr_dates[[\n",
    "    'pdc_pid',\n",
    "    'age_res_time',\n",
    "    'val_numeric'\n",
    "]]\n",
    "\n",
    "if debug_count: print('Number of rows of SCr values:',len(scr_dates))\n",
    "if debug_show: scr_dates.head()\n",
    "\n",
    "# Now we join this to the base cohort by PID and compute # (fraction) of days difference\n",
    "# between ICU admission and each SCr value\n",
    "old_scr_vals = base_cohort.merge(\n",
    "    scr_dates,\n",
    "    on='pdc_pid',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "old_scr_vals['n_age_days'] = old_scr_vals['ageatadmission'].astype('long')\n",
    "old_scr_vals['icu_admit_days'] = old_scr_vals['n_age_days'] + (old_scr_vals['n_icuadmit'] * (60. * 60. * 24.))\n",
    "old_scr_vals['res_before_admit'] = old_scr_vals['icu_admit_days'] - old_scr_vals['age_res_time']\n",
    "\n",
    "old_scr_vals = old_scr_vals[old_scr_vals['res_before_admit'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1709575389107
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "old_scr_vals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Above we generated a data frame which has multiple rows per unique base_cohort tuple (`hid`, `eid`), and each row has an SCr value and a `res_before_admit` value, which is the number of days before the ICU admisison that the result was reported.\n",
    "\n",
    "Here we filter by 0.5 days and 6 months, order by `res_before_admit` (ascending), and find the minimum value for each tuple (`hid`, `eid`). Importantly, the size (`len()`) of these rows will not be the same as the size of the base cohort since some tuples (`hid`, `eid`) will not have any values in the GROUP BY statement. Therefore we have to left join back onto the base cohort at some point (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_scr_vals = old_scr_vals.sort_values(['pdc_hid', 'pdc_eid', 'res_before_admit'])\n",
    "\n",
    "# We want lab results from 12 hours before admission to 180 days before admission\n",
    "lab_result_time_min = 0.5 * 24 * 60 * 60\n",
    "lab_result_time_max = 180 * 24 * 60 * 60\n",
    "\n",
    "old_scr_single_full = old_scr_vals[(old_scr_vals['res_before_admit'] >= lab_result_time_min) & (old_scr_vals['res_before_admit'] < lab_result_time_max)]\n",
    "\n",
    "first_col_rename_dict = {\n",
    "    'age_res_time': 'first_age_res_time',\n",
    "    'val_numeric': 'first_scr',\n",
    "    'res_before_admit': 'first_res_before_admit'\n",
    "}\n",
    "old_scr_single_first = old_scr_single_full[[\n",
    "    'pdc_hid', \n",
    "    'pdc_eid', \n",
    "    'sex',\n",
    "    'age_res_time',\n",
    "    'val_numeric',\n",
    "    'res_before_admit'\n",
    "]].groupby(['pdc_hid', 'pdc_eid', 'sex']).first().reset_index()\n",
    "\n",
    "for original_col in first_col_rename_dict.keys():\n",
    "    old_scr_single_first[first_col_rename_dict[original_col]] = old_scr_single_first[original_col].copy()\n",
    "    old_scr_single_first.drop(original_col, axis=1, inplace=True)\n",
    "    \n",
    "old_scr_single_min = old_scr_single_full[[\n",
    "    'pdc_hid', \n",
    "    'pdc_eid', \n",
    "    'sex',\n",
    "    'res_before_admit'\n",
    "]].groupby(['pdc_hid', 'pdc_eid', 'sex']).min().reset_index()\n",
    "\n",
    "old_scr_single_min['min_res_before_admit'] = old_scr_single_min['res_before_admit'].copy()\n",
    "old_scr_single_min.drop('res_before_admit', axis=1, inplace=True)\n",
    "\n",
    "old_scr_single = old_scr_single_first.merge(\n",
    "    old_scr_single_min,\n",
    "    on=['pdc_hid', 'pdc_eid', 'sex']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1709569339422
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "type(old_scr_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1709327787402
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if debug_count: print('Number of rows:', len(old_scr_single))\n",
    "if debug_show: old_scr_single.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(old_scr_vals[(old_scr_vals['res_before_admit'] >= lab_result_time_min) & (old_scr_vals['res_before_admit'] < lab_result_time_max)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.box(old_scr_vals[[\"site_id\", \"age_res_time\"]], x=\"site_id\", y=\"age_res_time\", \n",
    "    labels={\n",
    "        \"site_id\": \"Site ID\",\n",
    "        \"val_numeric\": \"Serum Creatinine Value\"\n",
    "    },\n",
    "    log_y=True)\n",
    "fig.update_xaxes(type='category')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Schwartz Values\n",
    "\n",
    "Below we import the values from Schwartz 1976, which are used to minimum-check the baseline SCr values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1709569496104
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "columns = ['age', 'sex', 'mean', 'sd']\n",
    "data = [\n",
    "    (1, 'Female', 0.35, 0.05), (1, 'Male', 0.41, 0.10),\n",
    "    (2, 'Female', 0.45, 0.07), (2, 'Male', 0.43, 0.12),\n",
    "    (3, 'Female', 0.42, 0.08), (3, 'Male', 0.46, 0.11),\n",
    "    (4, 'Female', 0.47, 0.12), (4, 'Male', 0.45, 0.11),\n",
    "    (5, 'Female', 0.46, 0.11), (5, 'Male', 0.50, 0.11),\n",
    "    (6, 'Female', 0.48, 0.11), (6, 'Male', 0.52, 0.12),\n",
    "    (7, 'Female', 0.53, 0.12), (7, 'Male', 0.54, 0.14),\n",
    "    (8, 'Female', 0.53, 0.11), (8, 'Male', 0.57, 0.16),\n",
    "    (9, 'Female', 0.55, 0.11), (9, 'Male', 0.59, 0.16),\n",
    "    (10, 'Female', 0.55, 0.13), (10, 'Male', 0.61, 0.22),\n",
    "    (11, 'Female', 0.60, 0.13), (11, 'Male', 0.62, 0.14),\n",
    "    (12, 'Female', 0.59, 0.13), (12, 'Male', 0.65, 0.16),\n",
    "    (13, 'Female', 0.62, 0.14), (13, 'Male', 0.68, 0.21),\n",
    "    (14, 'Female', 0.65, 0.13), (14, 'Male', 0.72, 0.24),\n",
    "    (15, 'Female', 0.67, 0.22), (15, 'Male', 0.76, 0.22),\n",
    "    (16, 'Female', 0.65, 0.15), (16, 'Male', 0.74, 0.23),\n",
    "    (17, 'Female', 0.70, 0.20), (17, 'Male', 0.80, 0.18),\n",
    "    (18, 'Female', 0.72, 0.19), (18, 'Male', 0.91, 0.17)\n",
    "]\n",
    "\n",
    "schwartz_df = spark.createDataFrame(data).toDF(*columns)\n",
    "schwartz_df = schwartz_df.withColumn('min_value', schwartz_df['mean'] - 2. * schwartz_df['sd'])\n",
    "\n",
    "if debug_show: schwartz_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now we find the integer age (in years) from the `old_scr_single` data frame, and marge on age and gender to get the minimum value. If the actual value is less than the minimum, use the minimum from Schwarz 1976. This is per Sanchez-Pinto 2016 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1709565485917
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_integer_age(age_val):\n",
    "    return int(min(max(numpy.round(age_val), 1), 18))\n",
    "\n",
    "baseline_scr_single = old_scr_single.copy()\n",
    "\n",
    "baseline_scr_single['age'] = [get_integer_age(age) for age in baseline_scr_single['first_age_res_time']]\n",
    "\n",
    "baseline_scr_single = baseline_scr_single.merge(\n",
    "    schwartz_df.selectExpr(\"age\", \"sex\", \"min_value AS schwartz_val\").toPandas(),\n",
    "    on=['age', 'sex'],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "baseline_scr_single['baseline_scr'] = numpy.where(\n",
    "    baseline_scr_single['first_scr'] < baseline_scr_single['schwartz_val'],\n",
    "    baseline_scr_single['schwartz_val'], # if true\n",
    "    baseline_scr_single['first_scr']     # else\n",
    ")\n",
    "baseline_scr_single['baseline_src'] = numpy.where(\n",
    "    baseline_scr_single['first_scr'] < baseline_scr_single['schwartz_val'],\n",
    "    'schwartz', # if true\n",
    "    'prior'     # else\n",
    ")\n",
    "\n",
    "baseline_scr_single = baseline_scr_single[[\n",
    "    'pdc_hid',\n",
    "    'pdc_eid',\n",
    "    'baseline_scr',\n",
    "    'baseline_src'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1709325081298
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if debug_count: print('Number of rows in baseline_scr_single:',len(baseline_scr_single))\n",
    "if debug_show: baseline_scr_single.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Height Dependent & Independent\n",
    "\n",
    "Lastly we need to calculate the height-dependent (when available) and height-independent values of SCr to be used as baselines, if the baseline values above do not exist (as we know they don't for a large portion of the cohort).\n",
    "\n",
    "To do this we use the Hessey 2017 paper, which includes equations for both height-dependent (from Schwartz/Furth 2009) and height-independent (from Hoste 2014). Both of these require an eGFR, which we use from a lookup table (if age <= 2) or 120 ml/min/1.73m2 if above the age of 2. The lookup table is from the Hessey 2017 article's supplementary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['age_min', 'age_max', 'egfr']\n",
    "\n",
    "data = [\n",
    "    (0., 0.1, 42),\n",
    "    (0.1, 0.3, 53),\n",
    "    (0.3, 0.66, 71),\n",
    "    (0.66, 1.00, 84),\n",
    "    (1., 1.5, 91),\n",
    "    (1.5, 2., 97),\n",
    "    (2., 100., 120)\n",
    "]\n",
    "\n",
    "# egfr_df = spark.createDataFrame(data).toDF(*columns)\n",
    "egfr_df = pd.DataFrame(\n",
    "    data=data,\n",
    "    columns=columns\n",
    ")\n",
    "\n",
    "if debug_show: egfr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the functions to return the height-dependent and height-independent values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def height_dependent(egfr, height_cm):\n",
    "    \"\"\"\n",
    "    Height-dependent metohd, from Schwartz/Furth 2009,\n",
    "    requires egfr and height (in cm).\n",
    "    \"\"\"\n",
    "\n",
    "    return (0.41 * height_cm) / egfr\n",
    "\n",
    "def height_independent_male(egfr, age_yrs):\n",
    "    \"\"\"\n",
    "    Height-independent method (male), from Hoste 2014,\n",
    "    requires egfr and age (in years)\n",
    "    \"\"\"\n",
    "\n",
    "    Q = 0.21 \\\n",
    "        + 0.057 * age_yrs \\\n",
    "        - 0.0075 * (age_yrs*age_yrs) \\\n",
    "        + 0.00064 * (age_yrs*age_yrs*age_yrs) \\\n",
    "        - 0.000016 * (age_yrs*age_yrs*age_yrs*age_yrs)\n",
    "    \n",
    "    return (107.3 * Q) / egfr\n",
    "\n",
    "def height_independent_female (egfr, age_yrs):\n",
    "    \"\"\"\n",
    "    Height independent method (Female), from Hoste 2014, \n",
    "    requires egfr & age (in years) \n",
    "    \"\"\"\n",
    "\n",
    "    Q = 0.23 \\\n",
    "        + 0.034 * age_yrs \\\n",
    "        - 0.0018 * (age_yrs*age_yrs) \\\n",
    "        + 0.00017 * (age_yrs*age_yrs*age_yrs) \\\n",
    "        - 0.0000051 * (age_yrs*age_yrs*age_yrs*age_yrs)\n",
    "\n",
    "    return (107.3 * Q) / egfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we gather the required elements - we need a data table with the `base_cohort` primary key tuple (`pdc_hid`, `pdc_eid`) as well as `age` and `sex`. From this we left join to `height` (if not `None`), and then we join to `egfr` to find baseline egfr for each of these. Lastly, we apply both height-dependent and height-independent functions to these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_baseline = base_cohort[[\n",
    "    'pdc_hid',\n",
    "    'pdc_eid',\n",
    "    'site_id',\n",
    "    'ageatadmission',\n",
    "    'sex'\n",
    "]]\n",
    "func_baseline['age_yrs'] = func_baseline['ageatadmission'] / 365.25\n",
    "func_baseline = func_baseline[func_baseline['age_yrs'] <= 18]\n",
    "\n",
    "if debug_count: print('Number of rows in base_cohort: %d' % len(func_baseline))\n",
    "\n",
    "func_baseline = func_baseline.merge(\n",
    "    height_join_lim_mod.toPandas(),\n",
    "    on=['pdc_hid', 'pdc_eid', 'site_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "if debug_count: print('Number of rows in base_cohort joined to heights: %d' % len(func_baseline))\n",
    "\n",
    "func_baseline['join_col'] = ['join_all' for i in range(len(func_baseline))]\n",
    "egfr_df['join_col'] = ['join_all' for i in range(len(egfr_df))]\n",
    "\n",
    "func_baseline = func_baseline.merge(\n",
    "    egfr_df,\n",
    "    on='join_col',\n",
    "    how='outer'\n",
    ")[[\n",
    "    'pdc_hid',\n",
    "    'pdc_eid',\n",
    "    'site_id',\n",
    "    'sex',\n",
    "    'age_yrs',\n",
    "    'age_min',\n",
    "    'age_max',\n",
    "    'val_height',\n",
    "    'egfr'\n",
    "]]\n",
    "\n",
    "func_baseline = func_baseline[(func_baseline['age_yrs'] >= func_baseline['age_min']) & (func_baseline['age_yrs'] < func_baseline['age_max'])]\n",
    "# func_baseline = func_baseline[(func_baseline['age_yrs'] >= func_baseline['age_min'])]\n",
    "# func_baseline = func_baseline[(func_baseline['age_yrs'] < func_baseline['age_max'])]\n",
    "\n",
    "func_baseline['val_height'] = func_baseline['val_height'].astype(float)\n",
    "\n",
    "if debug_count: print('Number of rows joined to egfr: %d' % len(func_baseline))\n",
    "\n",
    "if debug_show: func_baseline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_baseline.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we don't have any NULL egfr (and see how many NULL heights we have):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_count: \n",
    "    print('# of rows total:', len(func_baseline))\n",
    "\n",
    "    # print('# of rows with NULL height:',func_baseline.filter(isnull(col('val_height'))).count())\n",
    "    print('# of rows with NULL height:',len(func_baseline[func_baseline['val_height'].isna()]))\n",
    "\n",
    "    # print('# of rows with NULL egfr:',func_baseline.filter(isnull(col('egfr'))).count())\n",
    "    print('# of rows with NULL egfr:',len(func_baseline[func_baseline['egfr'].isna()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the height-dependent and height-independent functions across this cohort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_bscr_with_heights = func_baseline[~func_baseline['val_height'].isna()].copy()\n",
    "calc_bscr_without_heights_male = func_baseline[(func_baseline['val_height'].isna()) & (func_baseline['sex']=='Male')].copy()\n",
    "calc_bscr_without_heights_female = func_baseline[(func_baseline['val_height'].isna()) & (func_baseline['sex']=='Female')].copy()\n",
    "\n",
    "calc_bscr_with_heights['bscr'] = height_dependent(calc_bscr_with_heights['egfr'], calc_bscr_with_heights['val_height'])\n",
    "calc_bscr_with_heights['bscr_source'] = ['height_dependent' for i in range(len(calc_bscr_with_heights))]\n",
    "\n",
    "calc_bscr_without_heights_male['bscr'] = height_independent_male(calc_bscr_without_heights_male['egfr'], calc_bscr_without_heights_male['age_yrs'])\n",
    "calc_bscr_without_heights_male['bscr_source'] = ['height_independent' for i in range(len(calc_bscr_without_heights_male))]\n",
    "\n",
    "calc_bscr_without_heights_female['bscr'] = height_independent_female(calc_bscr_without_heights_female['egfr'], calc_bscr_without_heights_female['age_yrs'])\n",
    "calc_bscr_without_heights_female['bscr_source'] = ['height_independent' for i in range(len(calc_bscr_without_heights_female))]\n",
    "\n",
    "joined_bscr = pd.concat([calc_bscr_with_heights, calc_bscr_without_heights_male, calc_bscr_without_heights_female])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_baseline['sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(func_baseline[~func_baseline['sex'].isin(['Male', 'Female'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(calc_bscr_with_heights) + len(calc_bscr_without_heights_male) + len(calc_bscr_without_heights_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_count: print('Number of rows in joined_bscr table:', len(joined_bscr))\n",
    "\n",
    "# Summarize by type\n",
    "if debug_show:\n",
    "    mean_by_source = joined_bscr[['bscr_source', 'bscr']].groupby('bscr_source').mean().reset_index()\n",
    "    mean_by_source.rename(columns={'bscr_mean': 'bscr'}, inplace=True)\n",
    "    count_by_source = joined_bscr[['bscr_source', 'bscr']].groupby('bscr_source').count().reset_index()\n",
    "    count_by_source.rename(columns={'count': 'bscr'}, inplace=True)\n",
    "    mean_by_source.merge(\n",
    "        count_by_source,\n",
    "        on='bscr_source',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "if debug_show:\n",
    "    joined_bscr.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data to use in next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_bscr.to_csv(os.environ['AKI_DATA_PDC'] + 'joined_bscr.csv', index=False)\n",
    "scr_join_left_icu.to_csv(os.environ['AKI_DATA_PDC'] + 'scr_join_left_icu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
